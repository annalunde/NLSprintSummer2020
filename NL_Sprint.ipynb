{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL Sprint.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/annalunde/NLSprintSummer2020/blob/master/NL_Sprint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWDnD3sx4jDa"
      },
      "source": [
        "TO DO:\n",
        "- importere all data: DONE\n",
        "- finne signifikante parametere og dokumentere de som ikke er signifikante: DONE\n",
        "- bygge validation, test og training set and write to new file: DONE\n",
        "- normalisere all data: DONE\n",
        "- les opp på ETL pipeline, ML biblio for Google og Feature Importance Permutation(?) og dropout layer: DONE\n",
        "- bygge LSTM-modell, husk: noter kumulative/avg loss between every 10 epochs for training and validation data, if average loss is low on training but high on validation: indicates overfitting. Add dropout layer if it overfits: DONE\n",
        "- kjøre try&fail metode på modellen med forksjellige parameterverdier og teste med validation set. Start with some different models that lapses in size: 50, 100, 200: DONE\n",
        "- bygge en modell med alle parameterne: Kan se fil for hvilke som bidro mest, Feature Importance, Permutation: DROPPET, ettersom YR API'et ikke har de samme features\n",
        "- Kjør modell med kun WP dataen og se om det er endring: DONE, ble ikke bedre\n",
        "- Benytt Scikit learn sine funksjoner for cross-validation: DONE \n",
        "- utfør hyperparameter space search/randomized parameter search(?)/grid search: a way for us to set up parameters that can be tweaked, maybe do some random parameter search\n",
        "- Sjekk om standardisering påvirker resultatet: DONE\n",
        "- Få den live: sjekk notat\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiQnCld3CVcx",
        "outputId": "aeacd011-f36c-4b0b-a38b-0798f2a23900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "def import_weatherData():\n",
        "      \n",
        "      path = \"/content/drive/My Drive/Colab Notebooks/data/weather_forecast_utc.csv\"\n",
        "      raw_dataset = pd.read_csv(path, header=0, #iterator=True, chunksize=10000, skiprows=range(1, 2000000),\n",
        "                                usecols=[ 'datetime_forecast_utc', 'datetime_start_utc', 'SUB_AIR_PRESSURE', 'SUB_AIR_HUMIDITY', 'SUB_PRECIPITATION', 'SUB_CLOUD_COVER', 'windpark_zone'\n",
        "                                         ]) #, skiprows=range(1, 20000)) #nrows = 2000000\n",
        "\n",
        "      raw_dataset = raw_dataset.dropna()\n",
        "\n",
        "      return raw_dataset\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjUNoPrx4ILC"
      },
      "source": [
        "def import_historicalData():\n",
        "      \n",
        "      path = \"/content/drive/My Drive/Colab Notebooks/data/data.2018-10-31.93b9dcde-5e2e-11ea-8d9e-000d3a64d565.csv\"\n",
        "      raw_dataset = pd.read_csv(path, header=0, usecols=['timestamp', \n",
        "                                                         'ActivePower (Average)','WindSpeed (Average)'], sep=\",\", nrows = 2002500)\n",
        "      \n",
        "\n",
        "      raw_dataset = raw_dataset.dropna()\n",
        "\n",
        "      return raw_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTEk2MuDX1Tj",
        "outputId": "a57f0252-2de7-435a-b9f5-0a916fc8dfee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Check autocorrelation in weather data\n",
        "hist_data = import_historicalData()\n",
        "output = hist_data['ActivePower (Average)'].autocorr(lag=1)\n",
        "print(output)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.953996527263065\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF5U_NJH_2Na"
      },
      "source": [
        "def import_runtimePlan():\n",
        "\n",
        "      path = \"/content/drive/My Drive/Colab Notebooks/data/running_plan.2018-10-31.93b9dcde-5e2e-11ea-8d9e-000d3a64d565.csv\"\n",
        "      raw_dataset = pd.read_csv(path, header=0, usecols=[\n",
        "                                                         #'turbine','timestamp',\n",
        "                                                    'ActivePowerLimit'\n",
        "                                                    #, 'StateRun'\n",
        "                                                    ], nrows = 100000)\n",
        "      raw_dataset = raw_dataset.dropna()\n",
        "\n",
        "      return raw_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXgB0WwEHRLv"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "import numpy as np\n",
        "\n",
        "def normalize(data):\n",
        "  #x_array = np.array(data[colName])\n",
        "  #Normalize the data, has to be in NP array format\n",
        "  return preprocessing.normalize([data])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG1CswoEfJve"
      },
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import f_regression\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "\n",
        "def find_pearsonCorr(x,y):\n",
        "# pearson's correlation feature selection for numeric input and numeric output\n",
        "  corr, _ = pearsonr(x, y)\n",
        "  print('Pearsons correlation: %.3f' % corr)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeRAcCkSPBS4"
      },
      "source": [
        "from scipy.stats import spearmanr\n",
        "\n",
        "def find_spearmanCorr(x,y):\n",
        "# calculate spearman's correlation\n",
        "  corr, _ = spearmanr(x, y)\n",
        "  print('Spearmans correlation: %.3f' % corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjbisXkINhkQ"
      },
      "source": [
        "\n",
        "Dokumentasjon:\n",
        "- Vi har valgt å benytte forecasts to timer frem i tid\n",
        "- Antar at parken ikke er for stor, bruker kun \"WP\": whole park, derfor predikerer vi også kun for hver turbin\n",
        "- Totalt 47 turbiner, Viktig for å regne ut energi: Note that the dataset contains sensor data for Power (P) for each turbine, measured in KW (Kilowatts), while you will have to predict Energy (E), measured in KWh (Kilowatt-hours), for the whole park. In a time period with constant power, Energy can be calculated by the formula:\n",
        "\n",
        "  E = P * t, where P is Power in KW, t is time in hours.\n",
        "\n",
        "  One should summarize all turbines to get the overall energy.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOx42fRV0ACA"
      },
      "source": [
        "from datetime import datetime\n",
        "import csv\n",
        "from datetime import timedelta\n",
        "\n",
        "\n",
        "def right_timedates(datetime_startIn, datetime_forecastIn):\n",
        "      #Sjekker når værmeldingen er for og returnerer tidspunktet for en time senere\n",
        "      datetime_start = str(datetime_startIn)\n",
        "      datetime_forecast = str(datetime_forecastIn)\n",
        "      datetime_start = datetime.strptime(datetime_start, '%Y-%m-%d %H:%M:%S+00:00')\n",
        "      datetime_forecast = datetime.strptime(datetime_forecast, '%Y-%m-%d %H:%M:%S+00:00')\n",
        "      diff = datetime_start - datetime_forecast\n",
        "      return  diff  <= timedelta(hours = 1)  \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9gVJzfu79Av"
      },
      "source": [
        "import numpy as np\n",
        "def checkCorrelation_weather_and_histdata(weather_data, historical_data):\n",
        "  #Dok: Trenger weather_data: datetime_forecast_utc, datetime_start_utc, SUB_WIND_SPEED_10\n",
        "                #historical_data: timestamp, WindSpeed (Average), ActivePower (Average)\n",
        "  list_weather = []\n",
        "  list_hist = []  \n",
        "\n",
        "  #for row_w in weather_data:\n",
        "  for start_w in range (0,len(weather_data)-1):\n",
        "    row_w = weather_data.iloc[start_w:start_w+1]\n",
        "    if (row_w[\"windpark_zone\"][start_w] == \"WP\"):\n",
        "\n",
        "      if (right_timedates(row_w[\"datetime_start_utc\"][start_w],row_w[\"datetime_forecast_utc\"][start_w])):\n",
        "        path = \"/content/drive/My Drive/Colab Notebooks/data/data.2018-10-31.93b9dcde-5e2e-11ea-8d9e-000d3a64d565.csv\"\n",
        "        df = pd.read_csv(path, header=0, usecols=['timestamp', 'ActivePower (End)','WindSpeed (Average)'], sep=\",\", nrows = 2002500)\n",
        "        df = df.dropna()\n",
        "\n",
        "        start_date = str(row_w[\"datetime_start_utc\"][start_w]) \n",
        "        mask = (df[\"timestamp\"] == start_date)\n",
        "\n",
        "        df = df.loc[mask]\n",
        "        df.set_index(\"timestamp\", inplace = True) \n",
        "\n",
        "        if (df.empty):\n",
        "          continue\n",
        "\n",
        "        #for row_h in historical_data:\n",
        "        for start in range(0,len(df)-1):\n",
        "          row_h = df.iloc[start:start+1]\n",
        "\n",
        "          #Vi har en prediksjon to timer fram i tid og historical og weather er for ca. samme tidspunkt\n",
        "          weather_el = row_w[\"SUB_CLOUD_COVER\"][start_w]\n",
        "          #float(row_w[\"SUB_CLOUD_COVER\"][start_w] #\"\".replace(',',''))\n",
        "          list_weather.append(weather_el)\n",
        "          element = round(row_h[\"ActivePower (End)\"][start_date],1)\n",
        "          list_hist.append(element)\n",
        "          if (len(list_weather) == 1000):\n",
        "            print(list_weather)\n",
        "            print(list_hist)\n",
        "            normd_x = np.asarray(list_hist)\n",
        "            normd_y = np.asarray(list_weather)\n",
        "            normd_weather = normalize(normd_y)\n",
        "            normd_historical = normalize(normd_x)\n",
        "            normd_historical = normd_historical[0]\n",
        "            normd_weather = normd_weather[0]\n",
        "            find_pearsonCorr(normd_weather,normd_historical)\n",
        "            find_spearmanCorr(normd_weather,normd_historical)\n",
        "            return\n",
        "\n",
        "      else:\n",
        "        continue\n",
        "            \n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2ckoo0gvv0j"
      },
      "source": [
        "When checking for correlation in the above sets, whe have not factored in the Active Power Limit. This will be neccessary to fix before building the model. This is fixed under. Also, we try to implement ETL pipelining to some extent, so we end up writing to a new CSV file when we build the data sets under."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6t8Hu0-FSMRB"
      },
      "source": [
        "hist_data = import_historicalData()\n",
        "weather_data = import_weatherData()\n",
        "\n",
        "checkCorrelation_weather_and_histdata(weather_data, hist_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvoo2Qx-Vd4b",
        "outputId": "0934c9c7-9596-434e-8834-a3c51d3d1abd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import pandas as pd\n",
        "drive.mount(\"drive\", force_remount=True)\n",
        "\n",
        "def build_training_data_file(weather_data, historical_data):\n",
        "  #Bygger/skriver til filen som skal senere brukes til å dele inn i training, test og validation set\n",
        "  \n",
        "  #viktig: sjekk at filen er tom før skriver til den \n",
        "  #kan increase forecast tiden\n",
        " \n",
        "  #Dok: Trenger weather_data: datetime_forecast_utc, datetime_start_utc, SUB_AIR_PRESSURE, SUB_AIR_HUMIDITY, SUB_PRECIPITATION, SUB_CLOUD_COVER\n",
        "                #historical_data: timestamp, WindSpeed (Average), ActivePower (Average), ActivePowerLimit, StateRun\n",
        " \n",
        "  init_write = 0\n",
        "  #for row_w in weather_data:\n",
        "  for start_w in range (0, len(weather_data)):\n",
        "    row_w = weather_data.iloc[start_w:start_w+1]\n",
        "    if (not(row_w.empty)):\n",
        "\n",
        "      if (right_timedates(row_w[\"datetime_start_utc\"][start_w],row_w[\"datetime_forecast_utc\"][start_w])):\n",
        "        path = \"/content/drive/My Drive/Colab Notebooks/data/data.2018-10-31.93b9dcde-5e2e-11ea-8d9e-000d3a64d565.csv\"\n",
        "        df = pd.read_csv(path, header=0, usecols=['timestamp', 'ActivePower (Average)','WindSpeed (Average)'], sep=\",\") #nrows = 2002500)\n",
        "        df = df.dropna()\n",
        "\n",
        "        start_date = str(row_w[\"datetime_start_utc\"][start_w])\n",
        "        #viktig: må også sjekke at active power ikke er like active power limit (uten aggregate) og at stateRun = 1\n",
        "        mask = (df[\"timestamp\"] == start_date) & (df['StateRun'] == 1) & (df['ActivePower (Average)'] != df['ActivePowerLimit'])\n",
        "\n",
        "        df = df.loc[mask]\n",
        "        df.set_index(\"timestamp\", inplace = True) \n",
        "\n",
        "        if (df.empty):\n",
        "          continue\n",
        "        \n",
        "        print(row_w)\n",
        "\n",
        "        #for row_h in historical_data:\n",
        "        for start in range(0,len(df)):\n",
        "          row_h = df.iloc[start:start+1]\n",
        "          print(row_h,\"row h\")\n",
        "\n",
        "          #Vi har en prediksjon to timer fram i tid og historical og weather er for ca. samme tidspunkt, staterun = 1 og makser ikke limit på power\n",
        "          path = \"/content/drive/My Drive/Colab Notebooks/data/cleaned_data_withoutCloud.csv\"\n",
        "          #format: SUB_AIR_PRESSURE, SUB_AIR_HUMIDITY, SUB_PRECIPITATION, //SUB_CLOUD_COVER//, WindSpeed (Average), ActivePower (Average)\n",
        "          if (init_write == 0):\n",
        "            element = [row_w[\"SUB_AIR_PRESSURE\"][start_w],row_w[\"SUB_AIR_HUMIDITY\"][start_w] ,row_w[\"SUB_PRECIPITATION\"][start_w]  , row_h[\"WindSpeed (Average)\"][start_date], row_h[\"ActivePower (Average)\"][start_date]] \n",
        "            print(element, \"element\")\n",
        "            #element = [row_w[\"SUB_AIR_PRESSURE\"][start_w],row_w[\"SUB_AIR_HUMIDITY\"][start_w] ,row_w[\"SUB_PRECIPITATION\"][start_w] , row_w[\"SUB_CLOUD_COVER\"][start_w] , row_h[\"WindSpeed (Average)\"][start_date], row_h[\"ActivePower (Average)\"][start_date]] \n",
        "            insert_df = pd.DataFrame([element], columns=['SUB_AIR_PRESSURE', 'SUB_AIR_HUMIDITY', 'SUB_PRECIPITATION', 'WindSpeed (Average)', 'ActivePower (Average)'])\n",
        "            insert_df.to_csv(path, mode = 'a', index = False) \n",
        "            init_write = 1\n",
        "          else:\n",
        "            element = [row_w[\"SUB_AIR_PRESSURE\"][start_w],row_w[\"SUB_AIR_HUMIDITY\"][start_w] ,row_w[\"SUB_PRECIPITATION\"][start_w] , row_h[\"WindSpeed (Average)\"][start_date], row_h[\"ActivePower (Average)\"][start_date]] \n",
        "            insert_df = pd.DataFrame([element], columns=['SUB_AIR_PRESSURE', 'SUB_AIR_HUMIDITY', 'SUB_PRECIPITATION', 'WindSpeed (Average)', 'ActivePower (Average)'])\n",
        "            insert_df.to_csv(path, mode = 'a', header = False, index = False) \n",
        "                \n",
        "          \n",
        "          if (start_w == len(weather_data)-1):\n",
        "            return\n",
        "          \n",
        "          else:\n",
        "            continue \n",
        "\n",
        "      else:\n",
        "        continue\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJgklJtHVsRh"
      },
      "source": [
        "weather_data = import_weatherData()\n",
        "historical_data = import_historicalData()\n",
        "build_training_data_file(weather_data, historical_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPK_LSEa1WwJ"
      },
      "source": [
        "#for å splitte i test og train og normalisere data'en\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount(\"drive\", force_remount=True)\n",
        "import numpy as np\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/CleanedData/cleaned_data_whole.csv\"\n",
        "df = pd.read_csv(path,  engine='python')\n",
        "train, test = train_test_split(df, test_size=0.2, shuffle = False)\n",
        "\n",
        "X_train = train.drop(['ActivePower (Average)'], axis=1)\n",
        "y_train = train[['ActivePower (Average)']]\n",
        "X_test = test.drop(['ActivePower (Average)'], axis=1)\n",
        "y_test = test[['ActivePower (Average)']]\n",
        "\n",
        "\n",
        "y_test = y_test.values #returns a numpy array\n",
        "y_train = y_train.values\n",
        "min_max_scaler_y = preprocessing.MinMaxScaler()\n",
        "y_test_scaled = min_max_scaler_y.fit_transform(y_test)\n",
        "y_train_scaled = min_max_scaler_y.fit_transform(y_train)\n",
        "y_test = pd.DataFrame(y_test_scaled)\n",
        "y_train = pd.DataFrame(y_train_scaled)\n",
        "\n",
        "#convert air pressure to values\n",
        "X_train[\"SUB_AIR_PRESSURE\"] = X_train[\"SUB_AIR_PRESSURE\"].str.replace(',', '').astype(float)\n",
        "X_test[\"SUB_AIR_PRESSURE\"] = X_test[\"SUB_AIR_PRESSURE\"].str.replace(',', '').astype(float)\n",
        "\n",
        "X_train_np = X_train.values\n",
        "X_test_np = X_test.values\n",
        "\n",
        "#print(X_train_np, \"train before norm\")\n",
        "\n",
        "min_max_scaler_x = preprocessing.MinMaxScaler()\n",
        "x_train_normd = min_max_scaler_x.fit_transform(X_train_np)\n",
        "x_test_normd = min_max_scaler_x.fit_transform(X_test_np)\n",
        "x_train = pd.DataFrame(x_train_normd)\n",
        "x_test = pd.DataFrame(x_test_normd)\n",
        "\n",
        "\n",
        "print(x_train)\n",
        "print(x_test)\n",
        "\n",
        "#print(x_train.values.shape)\n",
        "#print(y_train.shape)\n",
        "#print(x_test.values.shape)\n",
        "#print(y_test.shape)\n",
        "#print(isinstance(x_train.values, np.ndarray))\n",
        "#time steps = 5\n",
        "#input sequence = 1\n",
        "#batch size = 447791\n",
        "#(447791, 5, 1)\n",
        "\n",
        "\n",
        "x_train = x_train.values.reshape((447796, 1, 5))\n",
        "x_test = x_test.values.reshape((111949, 1, 5))\n",
        "y_train = y_train.values.reshape((447796, 1, 1))\n",
        "y_test = y_test.values.reshape((111949, 1, 1))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rHSm0VRspt"
      },
      "source": [
        "#Save the scalers\n",
        "\n",
        "import joblib\n",
        "scalerX_filename = \"/content/drive/My Drive/Colab Notebooks/data/scalerX.save\"\n",
        "scalerY_filename = \"/content/drive/My Drive/Colab Notebooks/data/scalerY.save\"\n",
        "joblib.dump(min_max_scaler_x, scalerX_filename) \n",
        "joblib.dump(min_max_scaler_y, scalerY_filename) \n",
        "\n",
        "# And now to load...\n",
        "\n",
        "scaler = joblib.load(scaler_filename) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2GIM-iUtbAf"
      },
      "source": [
        "#for å splitte i test og train og STANDARDISERE data'en\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "drive.mount(\"drive\", force_remount=True)\n",
        "import numpy as np\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/CleanedData/cleaned_data_whole.csv\"\n",
        "df = pd.read_csv(path,  engine='python')\n",
        "df = df.dropna()\n",
        "\n",
        "X = df.drop(['ActivePower (Average)'], axis=1)\n",
        "y = df[['ActivePower (Average)']]\n",
        "\n",
        "#convert air pressure to values\n",
        "X[\"SUB_AIR_PRESSURE\"] = X[\"SUB_AIR_PRESSURE\"].str.replace(',', '').astype(float)\n",
        "\n",
        "values_X = X.values\n",
        "values_y = y.values\n",
        "# train the standardization\n",
        "scaler_X = StandardScaler()\n",
        "scaler = scaler_X.fit(values_X)\n",
        "scaler_y = StandardScaler()\n",
        "scaler = scaler_y.fit(values_y)\n",
        "# standardization the dataset \n",
        "y = scaler_y.transform(values_y)\n",
        "\n",
        "\n",
        "X = scaler_X.transform(values_X)\n",
        "\n",
        "X = pd.DataFrame(X)\n",
        "y = pd.DataFrame(y)\n",
        "\n",
        "print(X)\n",
        "print(y)\n",
        "\n",
        "x_train = X.values.reshape((559739, 1, 5))\n",
        "y_train = y.values.reshape((559739, 1, 1))\n",
        "\n",
        "# inverse transform and print the first 5 rows\n",
        "#inversed = scaler.inverse_transform(normalized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avssG-WGzeuZ"
      },
      "source": [
        "bygge LSTM-modell, husk: noter kumulative/avg loss between every 10 epochs for training and validation data, if average loss is low on training but high on validation: indicates overfitting. Add dropout layer if it overfits: DONE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go2xzt8KjhE1"
      },
      "source": [
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "accepted_diff = 0.01\n",
        "def linear_regression_equality(y_true, y_pred):\n",
        "    diff = K.abs(y_true-y_pred)\n",
        "    return K.mean(K.cast(diff < accepted_diff, tf.float32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHjFwg6ZzbMs"
      },
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import tensorflow_docs as tfdocs\n",
        "import tensorflow_docs.plots\n",
        "import tensorflow_docs.modeling\n",
        "import io\n",
        "from keras.layers import Dropout\n",
        "import h5py\n",
        "\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#defining a model to use\n",
        "model = tf.keras.Sequential()\n",
        "\n",
        "#LSTM layer\n",
        "model.add(tf.keras.layers.LSTM(200, activation='sigmoid',input_shape=(1, 5)))\n",
        "#Dense layer\n",
        "model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
        "#compiling model \n",
        "model.compile(optimizer='adam', loss='mse', metrics=[linear_regression_equality])\n",
        "\n",
        "# To avoid overfitting\n",
        "mc = tf.keras.callbacks.ModelCheckpoint(\"/content/drive/My Drive/Colab Notebooks/data/best_model\", monitor='val_linear_regression_equality', mode='max', save_best_only=True)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, verbose = 2)\n",
        "\n",
        "model.fit(x_train, y_train, epochs = 1000, batch_size =50,validation_split=0.2,  verbose = 2, callbacks=[early_stop, tfdocs.modeling.EpochDots(), mc])\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzyDdiaEfK8D"
      },
      "source": [
        "#To Export the model to serving format\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import h5py\n",
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "saved_model =  tf.keras.models.load_model(\"/content/drive/My Drive/Colab Notebooks/data/best_model_whole3.h5\", custom_objects={'linear_regression_equality': linear_regression_equality})\n",
        "\n",
        "import tempfile\n",
        "\n",
        "MODEL_DIR = tempfile.gettempdir()\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ucUHKVvrFL"
      },
      "source": [
        "#Cross validation:\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn import metrics\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import h5py\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "path = \"/content/drive/My Drive/Colab Notebooks/data/cleaned_data_whole.csv\"\n",
        "df = pd.read_csv(path)\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "X = df.drop(['ActivePower (Average)'], axis=1)\n",
        "y = df[['ActivePower (Average)']]\n",
        "\n",
        "y = y.values #returns a numpy array\n",
        "min_max_scaler_y = preprocessing.MinMaxScaler()\n",
        "y_scaled = min_max_scaler_y.fit_transform(y)\n",
        "y = pd.DataFrame(y_scaled)\n",
        "\n",
        "#convert air pressure to values\n",
        "X[\"SUB_AIR_PRESSURE\"] = X[\"SUB_AIR_PRESSURE\"].str.replace(',', '').astype(float)\n",
        "\n",
        "X = X.values\n",
        "min_max_scaler_x = preprocessing.MinMaxScaler()\n",
        "X = min_max_scaler_x.fit_transform(X)\n",
        "X = pd.DataFrame(X)\n",
        "\n",
        "\n",
        "#X = X.values.reshape((559739, 1, 5))\n",
        "#y = y.values.reshape((559739, 1, 1))\n",
        "\n",
        "def create_model():\n",
        "  model = Sequential()\n",
        "  model.add(tf.keras.layers.LSTM(200, activation='sigmoid',input_shape=(1, 5)))\n",
        "  model.add(tf.keras.layers.Dense(1,activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='mse', metrics=[linear_regression_equality])\n",
        "  return model\n",
        "\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "# evaluate using 10-fold cross validation\n",
        "model = KerasRegressor(build_fn=create_model, epochs=150, batch_size=10, verbose=2)\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "\n",
        "model = KerasRegressor(build_fn=create_model, epochs=150, batch_size=10, verbose=0)\n",
        "\n",
        "results = cross_val_score(model, X, y, cv=kfold)\n",
        "print(results.mean())\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPABd89cfYNU",
        "outputId": "506ac501-2729-4f1b-e74e-507791fdb9ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Example on loading and predicting further on a saved model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import h5py\n",
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "saved_model =  tf.keras.models.load_model(\"/content/drive/My Drive/Colab Notebooks/data/best_model_whole3.h5\", custom_objects={'linear_regression_equality': linear_regression_equality})\n",
        "\n",
        "arr = np.asarray([[0.5833,0.9166,0.,0.75,0.6668]])\n",
        "arr = arr.reshape((1,1,5))\n",
        "print(arr)\n",
        "#_, train_acc = saved_model.evaluate(trainX, trainy, verbose=0)\n",
        "\n",
        "\n",
        "\n",
        "pred = saved_model.predict(arr)\n",
        "print(pred)\n",
        "print(min_max_scaler_y.inverse_transform(pred))\n",
        "\n",
        "\n",
        "print(saved_model.evaluate(x_test, y_test))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[[[0.5833 0.9166 0.     0.75   0.6668]]]\n",
            "[[0.99928606]]\n",
            "[[4006.7896]]\n",
            "3499/3499 [==============================] - 5s 2ms/step - loss: nan - linear_regression_equality: 0.3799\n",
            "[nan, 0.37988531589508057]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvkraIpYjAos"
      },
      "source": [
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "[[0.9998485]]\n",
        "Epoch 143/10000\n",
        ".7165/7165 - 13s - loss: 0.0025 - linear_regression_equality: 0.5062 - val_loss: 0.0025 - val_linear_regression_equality: 0.5076\n",
        "\n",
        "LSTM: relu\n",
        "Dense: sigmoid\n",
        "Epoch 00019: early stopping\n",
        "[[0.72211695]]\n",
        "3499/3499 [==============================] - 4s 1ms/step - loss: 0.1131 - linear_regression_equality: 0.0084\n",
        "[0.11307872086763382, 0.008377393707633018]\n",
        "\n",
        "LSTM: sigmoid\n",
        "Dense: linear\n",
        "Epoch 00041: early stopping\n",
        "[[0.96833193]]\n",
        "3499/3499 [==============================] - 4s 1ms/step - loss: 0.0029 - linear_regression_equality: 0.3346\n",
        "[0.0029051369056105614, 0.33458369970321655]\n",
        "\n",
        "CONCLUSION: use sigmoid on both!\n",
        "\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Dropout: 0.2\n",
        "Epoch 00016: early stopping\n",
        "[[0.9976999]]\n",
        "3499/3499 [==============================] - 5s 1ms/step - loss: 0.0315 - linear_regression_equality: 0.1502\n",
        "[0.03149344399571419, 0.15018576383590698]\n",
        "\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Units = 50\n",
        "[[0.9992273]]\n",
        "Epoch 86/10000\n",
        ".7165/7165 - 15s - loss: 0.0024 - linear_regression_equality: 0.5814 - val_loss: 0.0024 - val_linear_regression_equality: 0.5764\n",
        "\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Units = 50\n",
        "Dropout: 0.2\n",
        "Epoch 00016: early stopping\n",
        "[[0.9966503]]\n",
        "3499/3499 [==============================] - 5s 1ms/step - loss: 0.0320 - linear_regression_equality: 0.1529\n",
        "[0.032032109797000885, 0.15288296341896057]\n",
        "\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Dropout: 0.2 \n",
        "Units = 100\n",
        "Epoch 00017: early stopping\n",
        "[[0.9976829]]\n",
        "3499/3499 [==============================] - 5s 1ms/step - loss: 0.0344 - linear_regression_equality: 0.1450\n",
        "[0.034427348524332047, 0.14495213329792023]\n",
        "\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Units = 200\n",
        "Epoch 92/10000\n",
        ".7165/7165 - 34s - loss: 0.0024 - linear_regression_equality: 0.5903 - val_loss: 0.0024 - val_linear_regression_equality: 0.5995\n",
        "\n",
        "\n",
        "WHEN ONLY USING WP DATA:\n",
        "Val_reg_eq_limit: 0.025\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Units = 200\n",
        "val_linear_regression_equality: 0.7995\n",
        "&\n",
        "Val_reg_eq_limit: 0.01\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Units = 200\n",
        "Epoch 128/1000\n",
        ".1434/1434 - 8s - loss: 0.0025 - linear_regression_equality: 0.5245 - val_loss: 0.0026 - val_linear_regression_equality: 0.5333\n",
        "CONCLUSION: Use whole data set\n",
        "\n",
        "Val_reg_eq_limit: 0.01, Saved model: whole2\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Epoch 61/1000\n",
        ".7165/7165 - 30s - loss: 0.0031 - linear_regression_equality: 0.5346 - val_loss: 0.0026 - val_linear_regression_equality: 0.6290\n",
        "Epoch 00061: early stopping, used whole data set\n",
        "\n",
        "Val_reg_eq_limit: 0.01, Saved model: whole3\n",
        "LSTM: sigmoid\n",
        "Dense: sigmoid\n",
        "Epoch 61/1000\n",
        ".7165/7165 - 30s - loss: 0.0031 - linear_regression_equality: 0.5346 - val_loss: 0.0026 - val_linear_regression_equality: 0.64\n",
        "Epoch 00061: early stopping, used whole data set\n",
        "\n",
        "GIKK FRA Å NORMALISERE TIL Å STANDARDISERE DATAEN: Det ble mye dråligere, dropper det\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxUZp9i4uTkY"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "def import_yrData():\n",
        "      \n",
        "      path_oslo = \"https://api.met.no/weatherapi/locationforecast/2.0/complete?lat=59.91&lon=10.71\"\n",
        "      path_stockholm = \"https://api.met.no/weatherapi/locationforecast/2.0/compact?lat=59.33&lon=18.07\"\n",
        "      path_koben = \"https://api.met.no/weatherapi/locationforecast/2.0/complete?lat=55.68&lon=12.57\"\n",
        "      path_munich = \"https://api.met.no/weatherapi/locationforecast/2.0/complete?lat=48.14&lon=11.59\"\n",
        "      \n",
        "      raw_dataset = pd.read_json(path_stockholm, orient ='columns')\n",
        "\n",
        "      #raw_dataset = raw_dataset.dropna()\n",
        "\n",
        "      return raw_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63J5U9uDu2zN"
      },
      "source": [
        "from pandas import Timestamp\n",
        "data = import_yrData()\n",
        "\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "from datetime import timedelta\n",
        "\n",
        "print(data)\n",
        "print(data.loc[\"timeseries\"][2])\n",
        "print(data.loc[\"timeseries\"][2][6])\n",
        "print(data.loc[\"timeseries\"][2][6]['time'])\n",
        "print(data.loc[\"timeseries\"][2][4]['data']['instant']['details'])\n",
        "\n",
        "#må hente ut riktig tidspunkt \n",
        "a = True\n",
        "init = 0\n",
        "while (a):\n",
        "  for i in range(1,1000):\n",
        "    tid = datetime.now().strftime(\"%Y-%m-%d %H:%M\")\n",
        "    ut = data.loc[\"timeseries\"][2][i]['time']\n",
        "    ut = ut.replace(\"T\", \" \").replace(\":00Z\", \"\")\n",
        "    \n",
        "    ut = datetime.strptime(ut, '%Y-%m-%d %H:%M')\n",
        "    tid = datetime.strptime(tid, '%Y-%m-%d %H:%M')\n",
        "    diff = ut - tid\n",
        "    if (diff <= timedelta(hours = 1)):\n",
        "      for k in range (1,49):\n",
        "          el_tid = data.loc[\"timeseries\"][2][k]['time'].replace(\"T\", \" \").replace(\":00Z\", \"\")\n",
        "          el = data.loc[\"timeseries\"][2][k][\"data\"][\"instant\"][\"details\"]\n",
        "          element_wind = el[\"wind_speed\"]\n",
        "          element_cloud = el[\"cloud_area_fraction\"]\n",
        "          el_p = data.loc[\"timeseries\"][2][k][\"data\"][\"next_1_hours\"][\"details\"]\n",
        "          element_precip = el_p[\"precipitation_amount\"]\n",
        "          #element_precip = (el_p[\"precipitation_amount_max\"] + el_p[\"precipitation_amount_min\"])/2\n",
        "          element_press = el[\"air_pressure_at_sea_level\"]\n",
        "          element_humid = el[\"relative_humidity\"]\n",
        "\n",
        "          element = [el_tid,element_press, element_humid, element_precip,element_cloud, element_wind]\n",
        "          element_ws = [el_tid,element_wind]\n",
        "          \n",
        "          path = \"/content/drive/My Drive/Colab Notebooks/data/yrAPI/yrAPI_Stockholm_5aug2.csv\"\n",
        "          path_wind = \"/content/drive/My Drive/Colab Notebooks/data/yrAPI/5.aug/ws_Stockholm_5aug2.csv\"\n",
        "\n",
        "          if (init ==0):\n",
        "            insert_df = pd.DataFrame([element], columns=['tid','SUB_AIR_PRESSURE', 'SUB_AIR_HUMIDITY', 'SUB_PRECIPITATION', 'SUB_CLOUD_COVER', 'WindSpeed (Average)'])\n",
        "            insert_df.to_csv(path, mode = 'a', header = True, index = False)\n",
        "            ws_insert = pd.DataFrame([element_ws], columns=['timestamp', 'WindSpeed (Average)'])\n",
        "            ws_insert.to_csv(path_wind, mode = 'a', header = True, index = False)\n",
        "            init = 1\n",
        "\n",
        "\n",
        "          else: \n",
        "            insert_df = pd.DataFrame([element], columns=['tid','SUB_AIR_PRESSURE', 'SUB_AIR_HUMIDITY', 'SUB_PRECIPITATION', 'SUB_CLOUD_COVER', 'WindSpeed (Average)'])\n",
        "            insert_df.to_csv(path, mode = 'a', header = False, index = False)\n",
        "            ws_insert = pd.DataFrame([element_ws], columns=['timestamp', 'WindSpeed (Average)'])\n",
        "            ws_insert.to_csv(path_wind, mode = 'a', header = False, index = False)\n",
        "            if (k == 48):\n",
        "              a = False\n",
        "\n",
        "          \n",
        "\n",
        "    \n",
        "#SUB_AIR_PRESSURE,SUB_AIR_HUMIDITY,SUB_PRECIPITATION,SUB_CLOUD_COVER,WindSpeed (Average)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZnVtzKI0h2n"
      },
      "source": [
        "#When predicting energy: remember to multiply by 47 and each turbine by their staterun factor\n",
        "\n",
        "# load a saved model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import h5py\n",
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "saved_model =  tf.keras.models.load_model(\"/content/drive/My Drive/Colab Notebooks/data/best_model_whole3.h5\", custom_objects={'linear_regression_equality': linear_regression_equality})\n",
        "\n",
        "path_yrAPI = \"/content/drive/My Drive/Colab Notebooks/data/yrAPI/yrAPI_Stockholm_5aug2.csv\"\n",
        "df = pd.read_csv(path_yrAPI)\n",
        "\n",
        "  #'tid','SUB_AIR_PRESSURE', 'SUB_AIR_HUMIDITY', 'SUB_PRECIPITATION', 'SUB_CLOUD_COVER', 'WindSpeed (Average)'\n",
        "for index,row in df.iterrows():\n",
        "  arr = np.asarray([[row['SUB_AIR_PRESSURE'],row['SUB_AIR_HUMIDITY'], row['SUB_PRECIPITATION'], row['SUB_CLOUD_COVER'], row['WindSpeed (Average)']]])\n",
        "  min_max_scaler_x.fit_transform(arr)\n",
        "  arr = arr.reshape((1,1,5))\n",
        "\n",
        "  pred = saved_model.predict(arr)\n",
        "  pred = min_max_scaler_y.inverse_transform(pred)\n",
        "\n",
        "  pred_scaled = 47*pred\n",
        " \n",
        "  #write to file\n",
        "  el = round(pred_scaled[0][0],2)\n",
        "  print(el)\n",
        "  element = [row['tid'], el]\n",
        "  path_pred = \"/content/drive/My Drive/Colab Notebooks/data/Predictions/5.aug/predictions_Stockholm_5aug2.csv\"\n",
        "  insert_df = pd.DataFrame([element], columns= ['timestamp', 'energy'])\n",
        "  insert_df.to_csv(path_pred, mode = 'a', index = False, header= False) \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CqbbjmSyWYM"
      },
      "source": [
        "path_yrAPI = \"/content/drive/My Drive/Colab Notebooks/data/yrAPI.csv\"\n",
        "df = pd.read_csv(path_yrAPI)\n",
        "\n",
        "\n",
        "for index,row in df.iterrows():\n",
        "  element = [row['tid'], row['WindSpeed (Average)']]\n",
        "  print(element)\n",
        "  pathws = \"/content/drive/My Drive/Colab Notebooks/data/ws.csv\"\n",
        "  insert_df = pd.DataFrame([element], columns= ['timestamp', 'energy'])\n",
        "  insert_df.to_csv(pathws, mode = 'a', index = False, header= False) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVH1rC4NS8li"
      },
      "source": [
        "#To connect to Google Cloud Platform:\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "CLOUD_PROJECT = 'teak-sunup-257603'\n",
        "BUCKET = 'gs://' + CLOUD_PROJECT + '-tf2-models'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fOXbfgbUhi1",
        "outputId": "cf2c38a8-a256-43a2-aa8c-ece71a6833c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!gcloud config set project $CLOUD_PROJECT\n",
        "!gsutil mb $BUCKET\n",
        "print(BUCKET)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "Creating gs://teak-sunup-257603-tf2-models/...\n",
            "gs://teak-sunup-257603-tf2-models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDcRan5FVPVG",
        "outputId": "3b7525b7-0164-4598-bdc8-7fd417027ce2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "from keras.models import load_model\n",
        "saved_model =  tf.keras.models.load_model(\"/content/drive/My Drive/Colab Notebooks/data/best_model\", custom_objects={'linear_regression_equality': linear_regression_equality})\n",
        "\n",
        "saved_model.save(BUCKET + '/custom_prediction_routine/greenforecast_model', save_format='tf')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "INFO:tensorflow:Assets written to: gs://teak-sunup-257603-tf2-models/custom_prediction_routine/greenforecast_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxKCj-NYWCep",
        "outputId": "2aacaaa2-11b3-454a-842a-c63383d9903f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "MODEL = 'greenforecast_model'\n",
        "!gcloud ai-platform models create $MODEL \\\n",
        " --regions=us-central1 \\\n",
        " --enable-logging #\\\n",
        " #--enable-console-logging"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using endpoint [https://ml.googleapis.com/]\n",
            "Created ml engine model [projects/teak-sunup-257603/models/greenforecast_model].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU67ZDcoWkqO",
        "outputId": "03c2077c-ffaf-40b1-b424-7a54f1af8ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "VERSION = 'v5'\n",
        "MODEL_DIR = BUCKET + '/custom_prediction_routine/greenforecast_model'\n",
        "!gcloud beta ai-platform versions create $VERSION \\\n",
        "  --model $MODEL \\\n",
        "  --origin $MODEL_DIR \\\n",
        "  --runtime-version=1.13 \\\n",
        "  --python-version=3.5 \\\n",
        "  --framework='tensorflow' \\\n",
        "  #--package-uris $BUCKET/custom_prediction_routine/my_custom_code-0.1.tar.gz \\\n",
        "  #--prediction-class predictor.MyPredictor\n",
        "\n",
        "\n",
        "#    --package-uris gs://{BUCKET}/{PACKAGES_DIR}/custom-model-0.1.tar.gz \\\n",
        "#    --prediction-class=custom_prediction.CustomModelPrediction \\\n",
        "#    --service-account custom@project_id.iam.gserviceaccount.com"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using endpoint [https://ml.googleapis.com/]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R424rT-0u2by"
      },
      "source": [
        "import googleapiclient.discovery\n",
        "\n",
        "def predict_json(project, model, instances, version=None):\n",
        "    \"\"\"Send json data to a deployed model for prediction.\n",
        "\n",
        "    Args:\n",
        "        project (str): project where the Cloud ML Engine Model is deployed.\n",
        "        model (str): model name.\n",
        "        instances ([Mapping[str: Any]]): Keys should be the names of Tensors\n",
        "            your deployed model expects as inputs. Values should be datatypes\n",
        "            convertible to Tensors, or (potentially nested) lists of datatypes\n",
        "            convertible to tensors.\n",
        "        version: str, version of the model to target.\n",
        "    Returns:\n",
        "        Mapping[str: any]: dictionary of prediction results defined by the\n",
        "            model.\n",
        "    \"\"\"\n",
        "    # Create the ML Engine service object.\n",
        "    # To authenticate set the environment variable\n",
        "    # GOOGLE_APPLICATION_CREDENTIALS=<path_to_service_account_file>\n",
        "    service = googleapiclient.discovery.build('ml', 'v1')\n",
        "    name = 'projects/{}/models/{}'.format(project, model)\n",
        "\n",
        "    if version is not None:\n",
        "        name += '/versions/{}'.format(version)\n",
        "\n",
        "    response = service.projects().predict(\n",
        "        name=name,\n",
        "        body={'instances': instances}\n",
        "    ).execute()\n",
        "\n",
        "    if 'error' in response:\n",
        "        raise RuntimeError(response['error'])\n",
        "\n",
        "    return response['predictions']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afvkvWmrwMqO"
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from json import JSONEncoder\n",
        "import numpy\n",
        "\n",
        "class NumpyArrayEncoder(JSONEncoder):\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, numpy.ndarray):\n",
        "            return obj.tolist()\n",
        "        return JSONEncoder.default(self, obj)\n",
        "\n",
        "instance = [0.5833,0.9166,0.,0.75,0.666]\n",
        "instance = arr.reshape((1,1,5))\n",
        "print(instance)\n",
        "\n",
        "# Serialization\n",
        "numpyData = {\"array\": instance}\n",
        "encodedNumpyData = json.dumps(numpyData, cls=NumpyArrayEncoder)  \n",
        "inst = json.dumps(np.array(instance).toList())\n",
        "\n",
        "# use dump() to write array into file\n",
        "print(\"Printing JSON serialized NumPy array\")\n",
        "print(encodedNumpyData)\n",
        "print(type(encodedNumpyData))\n",
        "\n",
        "test_predictions = predict_json(CLOUD_PROJECT, MODEL, inst)\n",
        "#print(min_max_scaler_y.inverse_transform(test_predictions))\n",
        "\n",
        "\n",
        "\n",
        "# Deserialization\n",
        "print(\"Decode JSON serialized NumPy array\")\n",
        "decodedArrays = json.loads(encodedNumpyData)\n",
        "\n",
        "finalNumpyArray = numpy.asarray(decodedArrays[\"array\"])\n",
        "print(\"NumPy Array\")\n",
        "print(finalNumpyArray)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}